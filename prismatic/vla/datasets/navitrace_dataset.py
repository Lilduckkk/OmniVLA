import os
import random
import glob
import torch
import numpy as np
from torch.utils.data import Dataset
from PIL import Image
from datasets import load_dataset
from typing import Tuple, Type, Dict
import math
import utm
import torchvision.transforms.functional as TF
import cv2

# OpenVLA/Prismatic ä¾èµ–
from prismatic.vla.constants import IGNORE_INDEX
from prismatic.models.backbones.llm.prompting import PromptBuilder
from prismatic.vla.action_tokenizer import ActionTokenizer
from transformers import PreTrainedTokenizerBase
from prismatic.models.backbones.vision import ImageTransform

class Navitrace_Dataset(Dataset):
    def __init__(
        self,
        action_tokenizer: PreTrainedTokenizerBase,
        base_tokenizer: ActionTokenizer,
        image_transform: ImageTransform,
        prompt_builder_fn: Type[PromptBuilder],
        data_root_dir: str = "data/data_splits/navitrace_dataset",
        data_split_type: str = "train",
        image_size: Tuple[int, int] = (224, 224),
        len_traj_pred: int = 8,
        predict_stop_token: bool = True,
        use_embodiment_prompt: bool = True,
        dataset_name="navitrace",
        len_context: int = 8,
    ):
        self.context_size = 5
        self.data_root_dir = data_root_dir
        self.data_split_type = data_split_type
        self.action_tokenizer = action_tokenizer
        self.base_tokenizer = base_tokenizer
        self.prompt_builder = prompt_builder_fn
        self.predict_stop_token = predict_stop_token
        self.image_transform = image_transform
        self.len_traj_pred = len_traj_pred  # ä¿å­˜è½¨è¿¹é•¿åº¦ç”¨äºç”Ÿæˆ mask
        self.MAX_TRAJECTORY_LENGTH = 20 # ç»Ÿä¸€è½¨è¿¹é•¿åº¦ä¸º20
        # 1. å®šä½ .arrow æ–‡ä»¶
        split_dir = os.path.join(self.data_root_dir, self.data_split_type)
        if not os.path.exists(split_dir):
            raise FileNotFoundError(f"Directory not found: {split_dir}")
        
        arrow_files = glob.glob(os.path.join(split_dir, "*.arrow"))
        if not arrow_files:
            raise FileNotFoundError(f"No .arrow files found in {split_dir}")
            
        # print(f"Loading raw arrow files from {split_dir}...")
        
        # 2. åŠ è½½æ•°æ®é›†
        self.dataset = load_dataset("arrow", data_files=arrow_files, split="train")

        # ------------------------------------------------------------------
        # 3. æ ¸å¿ƒä¿®æ”¹ï¼šæ„å»ºç´¢å¼•æ˜ å°„å¹¶è¿‡æ»¤æ— æ•ˆæ•°æ® (Filter Invalid Data)
        # ------------------------------------------------------------------
        self.index_map = []
        
        # åŒæ—¶åŠ è½½ embodiments å’Œ ground_truth åˆ—è¿›è¡Œæ£€æŸ¥
        all_embodiments = self.dataset['embodiments']
        all_ground_truths = self.dataset['ground_truth']
        
        # print("Building index map and filtering invalid trajectories...")
        skipped_count = 0

        # ä½¿ç”¨ zip åŒæ—¶éå†ï¼Œé¿å…åœ¨å¾ªç¯ä¸­åå¤æŸ¥è¯¢ dataset
        for row_idx, (embs_list, gt_dict) in enumerate(zip(all_embodiments, all_ground_truths)):
            if not embs_list:
                continue
                
            for emb_name in embs_list:
                # --- å…³é”®ä¿®å¤å¼€å§‹ ---
                # æ£€æŸ¥ ground_truth ä¸­æ˜¯å¦æœ‰è¯¥æœºå™¨äººçš„æ•°æ®ï¼Œä¸”æ•°æ®ä¸ä¸ºç©º
                is_valid = False
                if gt_dict and emb_name in gt_dict:
                    traj = gt_dict[emb_name]
                    # ç¡®ä¿è½¨è¿¹åˆ—è¡¨å­˜åœ¨ä¸”é•¿åº¦å¤§äº0
                    if traj is not None and len(traj) > 0:
                        is_valid = True
                
                if is_valid:
                    self.index_map.append((row_idx, emb_name))
                else:
                    skipped_count += 1
                # --- å…³é”®ä¿®å¤ç»“æŸ ---

        # print(f"Original samples: {len(self.dataset)}")
        # print(f"Skipped invalid/empty trajectories: {skipped_count}")
        print(f"Final Expanded samples: {len(self.index_map)}")
    
    def calculate_relative_position(self, x_a, y_a, x_b, y_b):
        return x_b - x_a, y_b - y_a

    def rotate_to_local_frame(self, delta_x, delta_y, heading_a_rad):
        rel_x = delta_x * math.cos(heading_a_rad) + delta_y * math.sin(heading_a_rad)
        rel_y = -delta_x * math.sin(heading_a_rad) + delta_y * math.cos(heading_a_rad)
        return rel_x, rel_y

    def _resize_norm(self, image, size):
        return TF.resize(image, size)
        
    def __len__(self):
        return len(self.index_map)

    def __getitem__(self, idx: int) -> Dict:

            thres_dist = 30.0
            metric_waypoint_spacing = 0.1 
            predict_stop_token=True

            original_row_idx, target_embodiment = self.index_map[idx]
            sample = self.dataset[original_row_idx]
            
            # åŸå§‹æ•°æ®
            task = sample['task']
            image = sample['image']
            current_image_PIL = image
            goal_image_PIL = image
            ground_truth = sample['ground_truth']
            # print(f"ground: {ground_truth}")
            # metadata = sample['metadata']

            # 3. ç”Ÿæˆé’ˆå¯¹ã€å½“å‰æœºå™¨äººã€‘çš„ä»»åŠ¡æè¿°
            embod_task = f"Generate the trajectory for {target_embodiment} to {task}."
            
            # æ–°å¢äº†ä¸€äº›æç¤ºï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼
            SYSTEM_PROMPT = """You are a navigation expert for various embodiments including robots and humans. Given an image of the current scenario, a specified embodiment (e.g., legged robot, wheeled robot, human, or bike), and a navigation task (e.g., "Go down the road"), you will predict a feasible future trajectory as a sequence of 2D points in normalized image coordinates (ranging from 0 to 1, where [0,0] is the top-left and [1,1] is the bottom-right).- The image shows a first-person view of the navigation scenario- Start your trajectory near the bottom center of the image, which corresponds approximately to normalized coordinate [0.5, 0.95] (representing the current position of the embodiment)- The trajectory should be adapted to the embodiment's abilities and limitations- Plan the path forward from this starting position based on what the embodiment can see and navigate- The trajectory should extend all the way to the goal if the path is visible. If the path is occluded, the trajectory should end where the path becomes fully obscured, unless the path can be reasonably inferred from the visible context.- If a red traffic light is visible and affects the planned path, or if crossing traffic or moving vehicles are present that make it unsafe to proceed, stop at an appropriate waiting position (e.g., just before the intersection or curb) and end the trajectory there.- All tasks that you are given have a solution- Output **only** the list of 2D points in normalized image coordinates (values between 0 and 1) in the following format: `[[x1, y1], [x2, y2], ..., [xn, yn]]`- Do not include any explanation or additional output### Embodiment Movement Characteristics- **Human**: A standard pedestrian. Can navigate stairs and ramps but cannot climb tall obstacles.- **Legged Robot**: A quadruped like ANYmal. Behaves similarly to a human, but it is shorter. It can handle stairs and escalators.- **Wheeled Robot**: A wheeled delivery robot. Behaves like a wheelchair, preferring smooth surfaces such as walkways and ramps. It cannot use stairs or escalators.- **Bicycle**: A standard cyclist. Follows traffic regulations and prefers bike lanes or streets. Cannot navigate stairs."""
            SYSTEM_PROMPT= """You are a navigation expert for various embodiments including robots and humans."""

            USER_PROMPT = """**Embodiment**: {embodiment}**Task**: {task}The image shows a first-person view from the embodiment's current position. Begin your trajectory near the bottom center of the image (around normalized coordinate [0.5, 0.95]) and predict the path forward as a list of 2D points in normalized coordinates (values from 0 to 1) according to the embodiment and the scenario shown in the image."""
            
            # 1. ğŸŒŸ æ„å»ºå®Œæ•´çš„æŒ‡ä»¤ Prompt (æ ¸å¿ƒä¿®æ”¹)
            # æ ¼å¼åŒ– user_prompt
            user_prompt_formatted = USER_PROMPT.format(embodiment=target_embodiment, task=task)
            
            # ç»„åˆå®Œæ•´çš„æŒ‡ä»¤
            full_instruction = SYSTEM_PROMPT + "\n\n" + user_prompt_formatted
            
            # å°†å®Œæ•´çš„æŒ‡ä»¤èµ‹ç»™ inst_obj/embod_task
            inst_obj = full_instruction
            embod_task = full_instruction

            # 4. å½’ä¸€åŒ–è½¨è¿¹æ•°æ®
            W, H = image.size
            
            # åˆå§‹åŒ–ä¸ºç©ºï¼Œä½†åœ¨ __init__ è¿‡æ»¤åï¼Œè¿™é‡Œç†è®ºä¸Šä¸€å®šä¼šæœ‰æ•°æ®
            normalized_traj = np.zeros((1, 4), dtype=np.float64) 
            original_normalized_traj_20 = np.zeros((self.MAX_TRAJECTORY_LENGTH, 4), dtype=np.float64)

            # ç›´æ¥è·å–æ•°æ®ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨ __init__ é‡Œä¿è¯äº†å®ƒå­˜åœ¨ä¸”ä¸ä¸ºç©º
            if target_embodiment in ground_truth:
                raw_trajs_list = ground_truth[target_embodiment]
                if len(raw_trajs_list) > 0:
                    raw_traj = np.array(raw_trajs_list[0]) # å–ç¬¬ä¸€æ¡ (åŸå§‹æ•°æ®)
                    
                    # å¤åˆ¶åŸå§‹æ•°æ®ç”¨äºå½’ä¸€åŒ–ï¼Œç¡®ä¿é•¿åº¦æ˜¯åŸå§‹é•¿åº¦ (e.g., 9 æ­¥)
                    if raw_traj.ndim >= 2 and raw_traj.shape[-1] >= 2:
                        # å½’ä¸€åŒ– x/W, y/H
                        traj_norm_orig = np.zeros_like(raw_traj, dtype=np.float64)
                        traj_norm_orig[:, 0] = raw_traj[:, 0] / W
                        traj_norm_orig[:, 1] = raw_traj[:, 1] / H
                        # è¡¥é›¶å˜æˆ 4 ç»´ (POSE_DIM)
                        original_normalized_traj = np.pad(traj_norm_orig, ((0, 0), (0, 2)), mode='constant', constant_values=0.0)
                    else:
                        # å¦‚æœç»´åº¦ä¸å¯¹ï¼Œåˆ›å»ºä¸€ä¸ªå ä½ç¬¦
                        original_normalized_traj = np.zeros((len(raw_traj) if raw_traj.ndim >= 2 else 1, 4), dtype=np.float64)
                        
                    # === [æ–°å¢é€»è¾‘] è§„å®š original_normalized_traj ä¸º 20 ä¸ªç‚¹ ===
                    current_length = original_normalized_traj.shape[0]
                    if current_length > self.MAX_TRAJECTORY_LENGTH:
                        # æˆªæ–­
                        original_normalized_traj_20 = original_normalized_traj[:self.MAX_TRAJECTORY_LENGTH, :]

                    elif current_length < self.MAX_TRAJECTORY_LENGTH:
                        # ä¸è¶³20æ­¥ï¼šç”¨æœ€åä¸€ä¸ªç‚¹å¡«å……ï¼ˆä¿æŒè½¨è¿¹è¿ç»­æ€§ï¼‰
                        num_pad = self.MAX_TRAJECTORY_LENGTH - current_length
                        last_point = original_normalized_traj[-1]  # å–æœ€åä¸€ä¸ªæœ‰æ•ˆç‚¹ï¼ˆ4ç»´ï¼‰
                        padding = np.tile(last_point, (num_pad, 1))  # é‡å¤num_padæ¬¡ï¼Œç”Ÿæˆå¡«å……æ•°æ®
                        original_normalized_traj_20 = np.vstack([original_normalized_traj, padding])

                    else:
                        original_normalized_traj_20 = original_normalized_traj
                    # === [æ–°å¢é€»è¾‘ç»“æŸ] ===
                    # å°†è·¯å¾„é•¿åº¦å›ºå®šä¸º8æ­¥
                    if len(raw_traj) < self.len_traj_pred:
                        # ä¸è¶³8æ­¥ï¼šç”¨æœ€åä¸€æ­¥çš„åæ ‡é‡å¤å¡«å……ï¼ˆä¿æŒè½¨è¿¹è¶‹åŠ¿ï¼Œæ¯”è¡¥é›¶æ›´åˆç†ï¼‰
                        num_pad = self.len_traj_pred - len(raw_traj)
                        last_point = raw_traj[-1]
                        padding = np.tile(last_point, (num_pad, 1))
                        raw_traj = np.vstack([raw_traj, padding])
                    elif len(raw_traj) > self.len_traj_pred:
                        # è¶…è¿‡8æ­¥ï¼šæˆªæ–­å‰8æ­¥
                        raw_traj = raw_traj[:self.len_traj_pred]
                        
                    # è®¡ç®— 8 æ­¥çš„ normalized_traj (ç”¨äº actions)
                    if raw_traj.ndim >= 2 and raw_traj.shape[-1] >= 2:
                        # å½’ä¸€åŒ– x/W, y/H
                        traj_norm = np.zeros_like(raw_traj, dtype=np.float64)
                        traj_norm[:, 0] = raw_traj[:, 0] / W
                        traj_norm[:, 1] = raw_traj[:, 1] / H 
                        # è¡¥é›¶å˜æˆ4ç»´
                        normalized_traj = np.pad(traj_norm, ((0, 0), (0, 2)), mode='constant', constant_values=0.0)
                    else:
                        raise ValueError(f"Unexpected empty trajectory after cropping for {target_embodiment} at index {original_row_idx}")
                
                else:
                    # å¦‚æœä»£ç è·‘åˆ°è¿™é‡Œï¼Œè¯´æ˜ __init__ è¿‡æ»¤é€»è¾‘æœ‰æ¼ç½‘ä¹‹é±¼ï¼ŒæŠ›å‡ºå¼‚å¸¸æ–¹ä¾¿è°ƒè¯•
                    raise ValueError(f"Unexpected empty trajectory list for {target_embodiment} at index {original_row_idx}")
            
            # modality_id = 7
            # modality_id = 8
            # åœ¨ 7 å’Œ 8 ä¹‹é—´éšæœºé€‰æ‹©
            modality_id = random.randint(7,8)
            inst_obj = embod_task
            actions = normalized_traj

            # # è™šæ‹Ÿå¯¼èˆªç›®æ ‡é€»è¾‘ (ä¿æŒä¸å˜)
            # current_lat, current_lon, current_compass = 37.87371258374039, -122.26729417226024, 270.0
            # cur_utm = utm.from_latlon(current_lat, current_lon)
            # cur_compass = -float(current_compass) / 180.0 * math.pi
            
            # goal_lat, goal_lon, goal_compass = 37.8738930785863, -122.26746181032362, 0.0
            # goal_utm = utm.from_latlon(goal_lat, goal_lon)
            # goal_compass = -float(goal_compass) / 180.0 * math.pi
            
            # delta_x, delta_y = self.calculate_relative_position(
            #     cur_utm[0], cur_utm[1], goal_utm[0], goal_utm[1]
            # )
            # relative_x, relative_y = self.rotate_to_local_frame(delta_x, delta_y, cur_compass)
            # radius = np.sqrt(relative_x**2 + relative_y**2)
            # if radius > thres_dist:
            #     relative_x *= thres_dist / radius
            #     relative_y *= thres_dist / radius
            # goal_pose_loc_norm = np.array([
            #     relative_y / metric_waypoint_spacing,
            #     -relative_x / metric_waypoint_spacing,
            #     np.cos(goal_compass - cur_compass),
            #     np.sin(goal_compass - cur_compass)
            # ])        

            # goal_pose_cos_sin = goal_pose_loc_norm 
            # print(" original_normalized_traj:", original_normalized_traj)
            current_x, current_y, current_compass = 0.5, 0.95, 0.0
            # é€†æ—¶é’ˆä¸ºæ­£
            goal_compass = np.arctan2(
                original_normalized_traj[-1, 1] - original_normalized_traj[-2, 1],  # Delta Y (å‚ç›´ä½ç§»)
                original_normalized_traj[-1, 0] - original_normalized_traj[-2, 0]   # Delta X (æ°´å¹³ä½ç§»)
            )
            goal_x, goal_y, goal_compass = original_normalized_traj[-1,0], original_normalized_traj[-1,1], goal_compass
            # print(f" goal_x:{goal_x} , goal_y:{goal_y} , goal_compass:{goal_compass} ")
            delta_x, delta_y = self.calculate_relative_position(
                current_x, current_y, goal_x, goal_y
            )     
            # print(f" delta_x:{delta_x} , delta_y:{delta_y} , current_compass:{current_compass} ")
            # print(" delta_x:", delta_x, " delta_y:", delta_y)
            relative_x, relative_y = self.rotate_to_local_frame(delta_x, delta_y, current_compass)    
            # print(" relative_x:", relative_x, " relative_y:", relative_y)   
            goal_pose_loc_norm = np.array([
                relative_x ,
                relative_y,
                np.cos(goal_compass - current_compass),
                np.sin(goal_compass - current_compass)
            ])             
            goal_pose_cos_sin = goal_pose_loc_norm
            # print(" goal_pose_cos_sin:", goal_pose_cos_sin)

            # æ„å»ºå¯¹è¯ Prompt
            IGNORE_INDEX = -100

            # äºŒæ¬¡å®‰å…¨æ£€æŸ¥
            if len(actions) == 0:
                # å¦‚æœè¿™é‡ŒæŠ¥é”™ï¼Œè¯´æ˜ä¸Šé¢çš„å½’ä¸€åŒ–é€»è¾‘æœ‰é—®é¢˜
                raise ValueError(f"Computed actions are empty for {target_embodiment} at index {original_row_idx}")

            current_action = actions[0]
            future_actions = actions[1:]
            future_actions_string = ''.join(self.action_tokenizer(future_actions))
            current_action_string = self.action_tokenizer(current_action)
            action_chunk_string = current_action_string + future_actions_string
            action_chunk_len = len(action_chunk_string)

            conversation = [
                {"from": "human", "value": f"{inst_obj}?"},
                {"from": "gpt", "value": action_chunk_string},
            ]

            prompt_builder = self.prompt_builder("openvla")
            for turn in conversation:
                prompt_builder.add_turn(turn["from"], turn["value"])

            # Tokenize
            input_ids = torch.tensor(self.base_tokenizer(prompt_builder.get_prompt(), add_special_tokens=True).input_ids)
            labels = input_ids.clone()
            labels[:-(action_chunk_len + 1)] = IGNORE_INDEX
            if not self.predict_stop_token:
                labels[-1] = IGNORE_INDEX

            # Images for MBRA model
            image_obs_list = []
            for ih in range(self.context_size + 1):
                image_obs_list.append(self._resize_norm(TF.to_tensor(current_image_PIL), (96, 96)))  #In our real code, image_obs_list is list of history image. In this dummy dataset code, we feed current images. The detail implementation is same as ViNT, NoMaD code base. 
            image_obs = torch.cat(image_obs_list)     
            image_goal = self._resize_norm(TF.to_tensor(goal_image_PIL), (96, 96))
            

            # è¿™é‡Œä¸ä¿®æ”¹å°ºå¯¸äº†ï¼ï¼ï¼ï¼æˆ‘éœ€è¦å®Œæ•´çš„image
            # Data augmentation (random cropping) 
            # voffset = int(224.0*0.2*random.random())
            # hoffset = int(224.0*0.1*random.random())            
            # PILbox = (hoffset, voffset, 224-hoffset, 224-voffset)
            # current_image_PIL = current_image_PIL.crop(PILbox).resize((224,224)) 
            # goal_image_PIL = goal_image_PIL.crop(PILbox).resize((224,224))   

            # å¯ä»¥ä¸ä¿®æ”¹å½¢çŠ¶ï¼Œä¿ç•™åŸå§‹çš„å½¢çŠ¶æ–¹ä¾¿å¯è§†åŒ–
            current_image_PIL = current_image_PIL.resize((1000, 1000))  # ç›´æ¥ä¿®æ”¹ä¸º224Ã—224
            goal_image_PIL = goal_image_PIL.resize((1000, 1000))        # ç›®æ ‡å›¾åŒæ­¥å¤„ç† 
            
            
            # æˆ‘ä»¬è¿™é‡Œçš„æ•°æ®æš‚æ—¶ä¸éœ€è¦ç¿»è½¬ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼å¦åˆ™ä¼šå‡ºé—®é¢˜
            # Data augmentation (horizontal flipping) 
            # if random.random() > 0.5:
            #     current_image_PIL = current_image_PIL.transpose(Image.FLIP_LEFT_RIGHT)
            #     goal_image_PIL = goal_image_PIL.transpose(Image.FLIP_LEFT_RIGHT)
            #     actions[:,1] = -actions[:,1]
            #     actions[:,3] = -actions[:,3]
            #     goal_pose_cos_sin[1] = -goal_pose_cos_sin[1]
            #     goal_pose_cos_sin[3] = -goal_pose_cos_sin[3]  
                
            #     image_obs = torch.flip(image_obs, dims=[2])
            #     image_goal = torch.flip(image_goal, dims=[2])  
                            
            pixel_values_current = self.image_transform(current_image_PIL)
            pixel_values_goal = self.image_transform(goal_image_PIL) 
            # pixel_values_current = self.image_transform(image)
            # pixel_values_goal = self.image_transform(image)
            # print(f"pixel_values_current shape: {pixel_values_current.shape}, pixel_values_goal shape: {pixel_values_goal.shape}")
            #action select 1.0: raw action, 0.0: MBRA synthetic action
            action_select_mask = torch.tensor(1.0)

            # ä¿æŒsegmentation_maskå½¢çŠ¶ä¸€è‡´
            segmentation_mask = sample['segmentation_mask']
            segmentation_mask = np.array(segmentation_mask, dtype=np.uint8)
            target_size = (1000, 1000)
            resized_mask_cv2 = cv2.resize(
                segmentation_mask,
                target_size,  # ä¿æŒä¸å›¾åƒä¸€è‡´çš„å¤§å°
                interpolation=cv2.INTER_NEAREST
            )
            # print(f"resized_mask_cv2 shape: {resized_mask_cv2.shape}, dtype: {resized_mask_cv2.dtype}")
            
            return {
                "sample_id": sample['sample_id'],
                "task": sample['task'],
                "embodiments": sample['embodiments'],
                "image": sample['image'],
                "segmentation_mask": torch.as_tensor(resized_mask_cv2),
                "ground_truth": sample['ground_truth'],
                "category": sample['category'],
                "context": sample['context'],
                "metadata": sample['metadata'],
                "embod_task": embod_task,
                "normalized_trajectory": normalized_traj,
                "original_normalized_trajectory": torch.as_tensor(original_normalized_traj_20),

                "pixel_values": pixel_values_current, # å»ºè®®åŠ ä¸Šè¿™ä¸ªé”®ï¼Œå¾ˆå¤šæ¨¡å‹é»˜è®¤è¯»å–è¿™ä¸ª
                "pixel_values_goal": pixel_values_goal,
                "input_ids": input_ids,
                "labels": labels,
                "dataset_name": "navitrace_dataset",
                "modality_id": modality_id,
                "actions": torch.as_tensor(actions),
                "action_select_mask": action_select_mask, # ä¿®å¤æœ¬æ¬¡æŠ¥é”™çš„å…³é”®
                "goal_pose": goal_pose_cos_sin, # ä¿®å¤ Proprio ç¼ºå¤±
                "obj_pose_norm": goal_pose_cos_sin[0:2],  # ä¿®å¤ Loss è®¡ç®—ç¼ºå¤±
                "img_PIL": current_image_PIL, 
                "gimg_PIL": goal_image_PIL,
                "cur_image":image_obs,
                "goal_image_8":image_goal,
                "temp_dist": 10.0,             # ä¿®å¤ Loss è®¡ç®—ç¼ºå¤±
                "lan_prompt": inst_obj
            }
